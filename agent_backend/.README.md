# Gemini-Powered Insurance Claims & Policy Agent

This repository contains the source code for an intelligent, conversational AI assistant designed to help customers with their insurance policies and claims. Built with Google Cloud's Vertex AI, including Gemini 2.5 Flash and Vector Search, this agent provides a professional, human-like interaction for handling policy inquiries, first notice of loss (FNOL), and complex policy comparisons.

## Table of Contents

- [Features](#features)
- [Architecture](#architecture)
- [Technology Stack](#technology-stack)
- [Setup and Deployment](#setup-and-deployment)
- [Testing](#testing)
- [Project Structure](#project-structure)
- [How to Contribute](#how-to-contribute)

---

## Features

-   **Conversational AI**: Utilizes the Gemini 2.5 Flash model for natural, context-aware dialogue. Avoids robotic phrases for a more human-like user experience.
-   **Advanced RAG (Retrieval-Augmented Generation)**: Employs Vertex AI Vector Search to retrieve the most relevant policy document sections for grounding LLM responses in factual data.
-   **Content-Based Policy Search**: Robustly identifies and validates policy numbers by searching the actual content of documents in Google Cloud Storage, overcoming OCR inconsistencies.
-   **Multi-Turn Intent Handling**: Intelligently analyzes user queries over multiple turns, restores context after interruptions (like asking for a policy number), and understands complex intents like FNOL, policy summaries, and comparisons.
-   **Dynamic Clarification**: Asks targeted, context-aware clarifying questions when necessary to gather details for a claim or specific coverage inquiry.
-   **Automated Referencing**: Dynamically generates citations in responses, linking information back to the specific source document and page number.
-   **Scalable Architecture**: The codebase is modularized into distinct services (analysis, search, document handling) and utilities, making it maintainable and easy to extend.

---

## Architecture

The application follows a standard Retrieval-Augmented Generation (RAG) pattern, orchestrated as a serverless function:

1.  **HTTP Trigger**: A Google Cloud Function receives the user's query via an HTTP POST request.
2.  **Intent Analysis**: The `AnalysisService` uses a hybrid regex-LLM approach to determine the user's intent (e.g., `policy_info`, `fnol`, `comparison`).
3.  **Policy Number Extraction**: The `parsers` utility extracts any potential policy numbers from the query and conversation history.
4.  **Mandatory Policy Gate**: The application enforces a critical check. If the intent requires a policy number and none is found, it immediately prompts the user for one.
5.  **Document Search**:
    -   If a policy number is present, the `SearchService` performs a targeted, content-based search across the document corpus in GCS to find chunks containing that specific policy number.
    -   For general queries, it performs a broad semantic search using Vertex AI Vector Search.
6.  **LLM Prompt Construction**: A detailed context, including the user's query, conversation history, and the retrieved document chunks, is compiled into a system prompt.
7.  **Response Generation**: The `LLMService` sends the prompt to the Gemini 2.5 Flash model to generate a conversational, accurate response.
8.  **Reference Generation**: The `ReferenceBuilder` utility adds inline citations to the response, linking back to the source documents.
9.  **JSON Response**: The final answer, along with references and session information, is returned to the client as a JSON object.

---

## Technology Stack

-   **Cloud Provider**: Google Cloud Platform (GCP)
-   **AI/ML Services**:
    -   Vertex AI Gemini 2.5 Flash (for language generation)
    -   Vertex AI Text Embedding Model (for creating vector embeddings)
    -   Vertex AI Vector Search (formerly Matching Engine) (for efficient similarity search)
-   **Backend**: Python 3.10+
-   **Framework**: Flask (for local testing), Google Functions Framework
-   **Storage**: Google Cloud Storage (for storing policy documents)

---

## Setup and Deployment

### Prerequisites

-   A Google Cloud Project with billing enabled.
-   The `gcloud` CLI installed and authenticated.
-   Python 3.10+ and `pip` installed.
-   A Google Cloud Storage bucket containing your processed policy documents.
-   A deployed Vertex AI Vector Search index endpoint.

### Configuration

1.  Clone the repository.
2.  Navigate to `config/settings.py`.
3.  Update the following constants with your specific GCP project details:
    -   `PROJECT_ID`
    -   `REGION`
    -   `INDEX_ENDPOINT_ID`
    -   `DEPLOYED_INDEX_ID`
    -   `BUCKET_NAME`

### Running Locally

1.  **Install Dependencies**:
    ```bash
    pip install -r requirements.txt
    ```

2.  **Set Google Application Credentials**:
    Ensure your local environment is authenticated to access Google Cloud services.
    ```bash
    gcloud auth application-default login
    ```

3.  **Run the Application**:
    The `app/main.py` file includes a local testing endpoint.
    ```bash
    python app/main.py
    ```
    The server will start on `http://0.0.0.0:8080`. You can send POST requests to the `/local_test` endpoint.

### Deploying to Google Cloud Functions

Deploy the `query_documents` function using the `gcloud` CLI from the root directory:

```bash
gcloud functions deploy query_documents \
--gen2 \
--runtime=python310 \
--region=YOUR_REGION \
--source=. \
--entry-point=query_documents \
--trigger-http \
--allow-unauthenticated